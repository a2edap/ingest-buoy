import logging
import re
from pathlib import Path
from typing import Dict, List, Pattern
from tsdat import PipelineConfig, read_yaml

logger = logging.getLogger(__name__)

__all__ = ["PipelineRegistry"]

class PipelineRegistry:
    """---------------------------------------------------------------------------------
    Registry of Pipelines that can be run on input keys.

    ---------------------------------------------------------------------------------"""

    def __init__(self):
        self._modules: List[str] = list()
        self._cache: Dict[Path, List[Pattern[str]]] = {}
        self._load()

    def dispatch(self, input_keys: List[str], clump: bool = False):
        """-----------------------------------------------------------------------------
        Instantiates and runs the appropriate Pipeline for the provided input files.
        according to the ingest's `mapping` specifications.

        Args:
            input_keys (List[str]]): A list of keys that the pipeline will process. Most
                of the time these will be path(s) to files on the local file system.
            clump (bool): A flag indicating if the dispatcher should use a single
                pipeline to process the input keys. If True, the first key will be used to
                determine the pipeline to run. Defaults to False.

        Returns:
            bool: True if the Pipeline ran without error, False otherwise.

        -----------------------------------------------------------------------------"""
        successes = 0
        failures = 0
        skipped = 0

        for input_key in input_keys:
            config_files = self._match_input_key(input_key)

            if len(config_files) > 1:
                raise RuntimeError(
                        f"More than one match for input key '{input_key}'. Please"
                        " update the pipeline triggers to remove duplicate matches."
                        f" Found matches: {config_files}"
                    )
            elif not len(config_files):
                logger.warn(
                    "No pipeline configuration found matching input key '%s'", input_key
                )
                skipped += 1
            else:
                config_file = config_files[0]

                config = PipelineConfig.from_yaml(config_file)
                pipeline = config.instantiate_pipeline()
                inputs = input_keys if clump else [input_key]
                logger.debug(
                    "Running pipeline %s on input %s",
                    pipeline.__repr_name__(),
                    inputs,
                )
                try:
                    pipeline.run(inputs)
                    if clump:
                        break
                except BaseException:
                    logger.exception(
                        "Pipeline '%s' failed to process input: %s",
                        pipeline.__repr_name__(),
                        inputs,
                    )
                    failures += 1
                else:
                    successes += 1
        
        logger.info(
            "Processing completed with %s successes, %s failures, and %s skipped.",
            successes,
            failures,
            skipped,
        )

    def _load(self, folder: Path = Path("pipelines")):
        """-----------------------------------------------------------------------------
        Discovers all pipelines under the parent module and registers them for later
        use.

        Args:
            folder (str, optional): The module (relative to the "ingest-awaken"
                folder) under which individual ingests live. Defaults to "ingest".

        -----------------------------------------------------------------------------"""
        config_paths = list(folder.glob("**/*pipeline*.yaml"))
        for path in config_paths:
            trigger_strs = read_yaml(path)["triggers"]
            triggers = [re.compile(trigger) for trigger in trigger_strs]
            self._cache[path] = triggers
            logger.debug(
                "Registered pipeline config '%s' with triggers %s", path, trigger_strs
            )

    def _match_input_key(self, input_key: str) -> List[Path]:
        """-----------------------------------------------------------------------------
        Matches the provided key to registered pipeline configuration filepaths.

        Args:
            input_key (str): The input key (most commonly the path to a file to
                process).

        Returns:
            List[Path]: The pathes to the pipeline configuration files whose triggers
                match the input key.

        -----------------------------------------------------------------------------"""

        matches: List[Path] = []
        for path, regex_list in self._cache.items():
            for regex in regex_list:
                if regex.match(input_key):
                    matches.append(path)
                    break
        return matches
        